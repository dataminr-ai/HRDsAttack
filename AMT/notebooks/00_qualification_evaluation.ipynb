{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Qualification Task Worker Evaluation\n",
    "* This notebook demonstrates how to calculate an aggregated accuracy score for each worker based on their submissions for the qualification task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import random\n",
    "import pprint\n",
    "import ujson\n",
    "from copy import deepcopy\n",
    "\n",
    "random.seed(4)\n",
    "\n",
    "from thefuzz import process, fuzz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fuzzy_score(target, candidate):\n",
    "    \"\"\"\n",
    "    return the fuzzy match score between two texts\n",
    "    the higher the more similar\n",
    "    \"\"\"\n",
    "    return fuzz.token_sort_ratio(target, candidate)\n",
    "\n",
    "\n",
    "def valid_checkpoint(answer):\n",
    "    \"\"\"\n",
    "    check if the worker copy/pasted the code during the checkpoint\n",
    "    \"\"\"\n",
    "    if type(answer['Answer.checkpoint_code_worker_input']) != str:\n",
    "        return False\n",
    "    return answer['Answer.checkpoint_code_gold_input'].strip() == answer['Answer.checkpoint_code_worker_input'].strip()\n",
    "\n",
    "\n",
    "def valid_selected_keys(selected_keys):\n",
    "    \"\"\"\n",
    "    consolidate the selected keys to pass into the next steps for worker evaluation\n",
    "    \"\"\"\n",
    "    \n",
    "    names = [[], [], []]\n",
    "    fuzzy = {}\n",
    "    other = {}\n",
    "    \n",
    "    for key_dict in selected_keys:\n",
    "        key = key_dict['key']\n",
    "        \n",
    "        if 'checkpoint_code' in key:\n",
    "            continue\n",
    "        \n",
    "        if 'Answer.victim_name' in key:\n",
    "            page_idx = int(key.split('_')[2])-1\n",
    "            names[page_idx].append(key_dict['value'])\n",
    "        else:\n",
    "            if key_dict['fuzzy']:\n",
    "                fuzzy[key] = key_dict['value']\n",
    "            else:\n",
    "                other[key] = key_dict['value']\n",
    "                \n",
    "    return names, fuzzy, other\n",
    "\n",
    "\n",
    "def answer_validation(answer, names, fuzzy, other, name_weight=2, verbose=False):\n",
    "    \"\"\"\n",
    "    takes in a worker submission and evaluate their performance based on the selected_keys\n",
    "\n",
    "    Params:\n",
    "        answer: Dict, key-value pairs from the AMT batch for a particular worker\n",
    "        names: List[List[String]], a list of names from each page\n",
    "        fuzzy: Dict, a dictionary of answers that use fuzzy matching to evaluate\n",
    "        other: Dict, a dictionary of answers on exact match\n",
    "        name_weight: Int, a weighting factor to scale up the accuracy contribution for finding correct names\n",
    "    Returns:\n",
    "        rejected: Boolean, if this worker should be rejected\n",
    "        accuracy: Float, accuracy for this worker\n",
    "    \"\"\"\n",
    "    rejected = True\n",
    "    checkpoint_failed = False\n",
    "    accuracy = 0\n",
    "    \n",
    "    acc_list = []\n",
    "    \n",
    "    gt_names = deepcopy(names)\n",
    "    answer_names = [[], [], []]\n",
    "    \n",
    "    #step 1: check checkpoint\n",
    "    if not valid_checkpoint(answer):\n",
    "        checkpoint_failed = True\n",
    "    \n",
    "    #step 2: check each key, validate names in the end\n",
    "    for key, val in answer.items():\n",
    "        if 'Answer.victim_name' in key and 'example' not in key:\n",
    "            page_idx = int(key.split('_')[2])-1\n",
    "            answer_names[page_idx].append(val)\n",
    "    \n",
    "        else:\n",
    "            \n",
    "            if key in fuzzy:\n",
    "                res = get_fuzzy_score(val, fuzzy[key]) >= 85\n",
    "                if verbose:\n",
    "                    print(key, val, fuzzy[key], res)\n",
    "                \n",
    "                acc_list.append(res)\n",
    "            elif key in other:\n",
    "                res = str(val) == str(other[key])\n",
    "                if verbose:\n",
    "                    print(key, val, other[key], res)\n",
    "                \n",
    "                acc_list.append(res)\n",
    "    \n",
    "    #step 3: validate names\n",
    "    for names_on_page, gt_names_on_page in zip(answer_names, gt_names):\n",
    "        names_on_page = [i for i in names_on_page if type(i)==str]\n",
    "        if verbose:\n",
    "            print(names_on_page, gt_names_on_page)\n",
    "        for cadi_name in names_on_page:\n",
    "            found_match = [False]*name_weight\n",
    "            for gt_name in gt_names_on_page:\n",
    "                if get_fuzzy_score(cadi_name, gt_name) >= 85:\n",
    "                    found_match = [True]*name_weight\n",
    "                    gt_names_on_page.remove(gt_name)\n",
    "                    break\n",
    "            acc_list += found_match\n",
    "    \n",
    "    accuracy = round(sum(acc_list)/len(acc_list), 2)\n",
    "    if verbose:\n",
    "        print(acc_list)\n",
    "    return accuracy < .4, accuracy, checkpoint_failed\n",
    "\n",
    "def eval_qa_batch(df_path, selected_keys):\n",
    "    \"\"\"\n",
    "    load an annotation batch from AMT and calculate worker performance\n",
    "    \"\"\"\n",
    "    qa_batch_df = pd.read_csv(df_path)\n",
    "    answers = qa_batch_df.to_dict('records')\n",
    "    \n",
    "    names, fuzzy, other = valid_selected_keys(selected_keys)\n",
    "    \n",
    "    for answer in answers:\n",
    "        answer['should_reject'], answer['accuracy'], answer['checkpoint_failed'] = answer_validation(answer, names, fuzzy, other)\n",
    "        \n",
    "    new_batch_df = pd.DataFrame(answers)\n",
    "    \n",
    "    res_df_path = df_path.split('.csv')[0]+'_evaluated.csv'\n",
    "    \n",
    "    new_batch_df.to_csv(res_df_path)\n",
    "    print(\"Evaluation results saved to {}\".format(res_df_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1. Processing ground truth annotations\n",
    "* To calculate the accuracy, we first manually submit ONE qualification HIT in the sandbox environment using the ground truth answers, then download the response.\n",
    "* The annotated data is stored in `Batch_360680_batch_results_gt_1.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_df_name = 'Batch_360680_batch_results_gt_1.csv'\n",
    "gt_df = pd.read_csv(os.path.join('../dataframes', gt_df_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(gt_df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2. Select the keys (columns) that should be included in the calculation\n",
    "* There are more than 200 columns in the annotated dataframe, only the ones corresponding to the attributes should be included in the calculation.\n",
    "* We first compile all the keys into a dataframe, then put the dataframe into a GoogleSheet, then manually mark the keys to be included"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_answer = gt_df.to_dict('records')[0]\n",
    "metric_df = pd.DataFrame()\n",
    "keys, vals = [], []\n",
    "for key, val in gt_answer.items():\n",
    "    keys.append(key)\n",
    "    vals.append(val)\n",
    "\n",
    "metric_df = pd.DataFrame({\n",
    "    'key': keys,\n",
    "    'value': vals\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_df.to_csv('../dataframes/metric_df_v1.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3. Edit in google sheet then load the dataframe to get the selected keys\n",
    "* Here we provide the edited dataframe in `../dataframes/qualification_metric_keys.csv`;\n",
    "* All the selected keys are stored in `../util_data/qualification_metric_key_values.json`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load it back\n",
    "qa_keys_df = pd.read_csv('../dataframes/qualification_metric_keys.csv')\n",
    "selected_keys = qa_keys_df[qa_keys_df.check].to_dict('records')\n",
    "for key_dict in selected_keys:\n",
    "    if key_dict['value'] == 'TRUE':\n",
    "        key_dict['value'] = True\n",
    "with open('../util_data/qualification_metric_key_values.json', 'w') as json_file:\n",
    "    ujson.dump(selected_keys, json_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4. Run evaluation functions\n",
    "* The `eval_qa_batch` function enclose all the steps required to evaluate a batch of qualification HITs.\n",
    "* Refer to `valid_selected_keys` and `answer_validation` for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test using the ground truth df\n",
    "# the resulting df should contain an accuracy of 1 \n",
    "# meaning a perfect score, located in the last three columns\n",
    "qa_batch_df_name = '../dataframes/Batch_360680_batch_results_gt_1.csv'\n",
    "eval_qa_batch(qa_batch_df_name, selected_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# similarly, applying the evaluation process to\n",
    "# other qualification batches as following:\n",
    "eval_qa_batch('../dataframes/Batch_4796448_batch_results_pilot_qualification_3_2nd.csv', \n",
    "              selected_keys)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
